Reinforcement learning is an important type of Machine Learning where an agent learn how to behave in a environment by performing actions and receving rewards.


The Reinforcement Learning (RL) process can be modelled as a loop that works like this:
-> Our Agent receives state S0 from the Environment
-> Based on that state S0, agent takes an action A0
-> Environment transitions to a new state S1. Environment gives some reward R1 to the agent.
This RL loop outputs a sequence of state, action and reward.

The goal of the agent is to maximize the expected cumulative reward. This the agent's goal because RL is based on the idea of the reward hypothesis: "All goals can be described by the maximization of the expected cumulative reward".

Hence, in RL, we find the best behaviour by maximising expected cumulative reward.

The "expected" cumulative reward at each timestep t (each RL loop) can be written as:

G_t = R_(t+1) + R_(t+2) + ...
    = (T)SUM(k=0) R_(t+k+1)

Which gives back the total reward for each future action per timestep until the end of simulation.

We can discount future rewards to control the ratio on caring between short term and long term rewards. We create a discount rate called "gamma".

The larger the gamma, the smaller the discount. So the agent cares about the long term reward nearer to as much as the short term reward.
The smaller the game, the bigger the discount, and the agent cares about the short term reward more.

Adding this discount variable, we get the following equation for our expected cumulative reward function G:

G_t = (T)SUM(k=0) gamma^(k) * R_(t+k+1)
where gamma is [0, 1)

Each reward will be discounted by gamma to the exponent of the time step, meaning we will always care less about long term reward, but to what degree, is dictated by gamma (especially because gamma != 1, and cannot negate the effect of the exponential).




There are two types of learning:
1) Monte Carlo - Collecting the rewards at the end of the episode and then calculating the maximum expected future reward.

For each run/episode, the agent will get rewards for each action/state it experiences in that episode. Depending on much reward it got in the future from being in the current state, it will update the estimated value of the state by stepping closer to the cumulative reward it received after being in this state.

2) Temporal Difference Learning - Estimate the rewards at each step.

This method does not wait until the end of the episode and update the value (maximum expected future reward) for a state after each step.
This method is called TS(0) which updates a value function after a single step.
The estimated value is stepped closer to the TD target, which replaces the total cumulative reward G from the Monte Carlo method. 
The TD target is obtained by combining the immediate reward and the discounted expected reward of the next state (so the next state the agent may be in after taking an action).
So we can update the state by taking an action in that state and gaining "experience" from it.




We need to handle the exploration/expoitation trade-off to find bigger rewards for the agent.




There are 3 approaches to solving an RL problem:
1) Value Based
Value Based RL aims to optimise the value function v(s), to achieve the agent's goal of maximising cumulative reward. 
The value function dictates the value of the state which represents the maximum expected future reward the agent will get at each state.

v_pi(s) = E_pi [ R_(t+1) + gamma*R_(t+2) + (gamma^2)*R_(t+3) + ... | S_t = s ]

We can calculate the value of state s under policy pi, by calculating the expected value after sampling all actions under policy pi. 

(WIKI "The expected value of a discrete random variable is the probability-weighted average of all possible values")

So depending on the actions taken through policy pi, we can calcualte an expected value for a total of all future rewards, with long term rewards having a discount factor.

We calculate the expected value given that we are in state s.

NOTE Q-Learning is value based because it learns a value function, and it performs policy iteration in order to do this. It does not learn a new policy, though the policy will become optimal as it acts on optimal state value estimates.

2) Policy Based
Aims to directly optimise the policy function pi(s), without using a value function.
The policy is what defines the agent's behaviour at a given time, such that 
a = pi(s).

We learn a policy function, which will allow the agent to map each state to the best corresponding action.

There are two types of policies:
-> Deterministic: a policy at a given state has its action dictate according to the state.
-> Stochastic: an action is chosen over a probability distribution.
E.g. pi(a | s) = P(A_t = a | S_t = s)
the action to take under policy (stochastic) pi given state s, is randomly distributed among all actions that are possible when in state s.

3) Model Based
Aims to model the environment, including the model of the behaviour of the environment. This usually means that each problem will need its own model representation though.




Policy Gradient (Value-Based)
---------------
Attempt to learn functions with directly map a state to an action, in order to find the best state to be in, i.e. learn the value of the state to be in, and performs an action to be in said state.


Q-Learning (Value-Based)
----------
NOTE max Q'(s', a') return the action with the highest q-value from this new state s'. The maximum action-value is always picked (no matter our policy, usually epsilon-greedy, though greedy policy would achieve act in the same way as this "max" term anyway), which is why Q-Learning is known as off-policy learning.
https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning

It is a tabular (consisting of or presented in columns or tables) learning method, we take a system of discrete states and discrete actions, and associate them with expected future rewards to determine the agent's behaviour. 

Many environments we deal with will be a continuous space, e.g. the cartpole environment where the cart will move along the x-axis, but we can construct a discrete space by making intelligent guesses about the parameter space.

See cartpole example for Q-Learning implementation.

Deciding which types of input and feedback your agent should pay attention to is a hard problem to solve. This is known as domain selection. Algorithms that are learning how to play video games can mostly ignore this problem, since the environment is man-made and strictly limited.

Since those actions are state-dependent, what we are really gauging is the value of state-action pairs. The value and meaning of an action is contingent upon the state in which it is taken. E.g. if the action is marrying someone, then marrying a 35-year-old when you’re 18 probably means something different than marrying a 35-year-old when you’re 90, and those two outcomes probably have different motivations and lead to different outcomes.

The Q function takes as its input an agent’s state and action, and maps them to probable rewards.


Q-Learning works by updating the expected cumulative reward of an action through experience of executing said action. From an intialised q-value table, as the agent traverses through a path from executing actions under a policy (probably epsilon-greedy), it will received a reward for each action in a state. 

Q[state][act] += ALPHA*(reward + GAMMA*max_q_s1a1 - Q[state][act])

Then, for each action it calculates the difference between the value of:
the reward at the current timestep and the expected cumulative reward of the next action-state pair (under the greedy policy)
AND 
the current expected cumulative reward of the state
it will then update this expected cumulative reward of the state by stepping through using the difference, since the reward + expected cumulative reward of the new state, is a better (or atleast as good) estimate of the reward of the current state
THUS
as the agent traverses many runs, it will have a q-table that will allow it behave optimally (if exploration and exploitiation is good, then the q-table will accurate and near complete!)

EXAMPLES: see implementations for cartpole and frozenlake.


Q-Network
---------
Q-Learning produces and updates a Q-table which an agent uses to find the best action to take given a state. But, producing this table can become ineffective in big state space environments, i.e. using a Q-learning function is not very scalable.

Tradition Q-Learning creates a Q table that holds the action-value(reward) for each state and action, i.e. q-value. 
Q-Networks uses a NN that takes a state and approximates Q-values for each action available, based on that state.

Using this model, we can create an agent to behave in complex environments. 

(Sf2 looks like a big state space environment, so maintaining a Q-table for an environment that has potentially millions of different states would not be efficient at all.)

Like all neural networks, they use coefficients to approximate the function relating inputs to outputs, and their learning consists to finding the right coefficients, or weights, by iteratively adjusting those weights along gradients that promise less error.

By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.

Instead of discretizing the input space, which may seem like a pretty hacky solution to this problem that we’ll be encountering over and over in future situations, we can simply apply neural nets to the situation instead! That’s where the D in DQN comes from... i guess this term is interchangeable as long as we use NN in some form.


EXAMPLE: FrozenLake

Where our input to the network is the 1x16 vector, this is a one-hot vector indicating the position of the player. There are 16 possible states (so 16 possible inputs), which our network will use produce 4 Q-values, one for each possible action.

One-hot vector is a vector with a single high (1) and all others low (0).

The method of updating is a little different as well, instead of updating a table of q-values, the network will be using backpropagation and a loss function to update the weight connections between layers of the network.

The cost function will be the "mean squared error" where the difference between the current predicted Q-values, and the "target" value is computed (as well as the gradiente of the error along the network).

The predicted Q-value will be the q-value our network predicts the action will reward from the inputted state.

The Q-target for an action is equivalent to the following equation:
Q[state][act] += ALPHA*(reward + GAMMA*max_q_s1a1 - Q[state][act])

THOUGH does this mean that the weight connection is only updated for the action taken, because we will only know the "immediate reward" for this action, and so we can update the weight connection associated with this action q-value.

The cost function would then be:
SUM (Q-target - Q)^2

In a similar way, the Q-target will be the new best (or atleast good) estimate of the q-value for an action.

SEE script_q_network.pay

While the network learns to solve the problem, it is less efficient than standard Q-learning. The Q-network has many extensions to allow for better performance and more robust learning. Two particular extensions are "Experience Replay" and "Freezing Target Networks". 

We will however move to using keras to setup our system, which we will see in the next example.


EXAMPLE MountainCar

Creating a Q-network will be difficult for certain environments because we can't necessarily generate training data for our NN to train on (i mean we can possibly run a random agent to gather some training data, but often it may not allow for convergence to optimal results).

We have to figure out a way to incrementally improve upon previous trials, which is in line with what Q-networks are all about.

The MSE (mean squared error) cost function will govern our network to train.

SUM (Q-target - Q)^2
=
SUM (reward + GAMMA*max_q_s1a1  -  Q(s, a))

which looks familiar from the standard q-learning update equation, we can see that the difference calculations in both equations are the same.

Q represents the value estimated by our model given the current state "s" and action taken "a", we know the action by following our policy (probably epsilon-greedy).

The network revolves around continuous learning, meaning that we do not need to accrue a bunch of training data and feed it into the model. Instead, training data is created through completign numerous trials.

SEE MountainCar/DQN.py for annotations on implementation

A key component of the DQN is the "memory" varaible (type deque) which is used to continuously train the model. However, rather than training on recent states the agent experiences, we add these states to memory and train on a random sample of that memory. 
We do this, instead of using the last x timesteps as the sample, because the training would only learn from most recent actions and may not be directly relevant for future predictions.
"In this environment in particular, if we were moving down the right side of the slope, training on the most recent trials would entail training on the data where you were moving up the hill towards the right. But, this would not be at all relevant to determining what actions to take in the scenario you would soon be facing of scaling up the left hill."
So, taking random samples means we don't bias our training set and learn about acting in all states of the environment we would encounter equally well.

Another key component in our DQN initialisationis the creation of the models. We create two models: the standard model and the target model. The target model is a "hack" implemented by DeepMind to improve convergence.

The role of the standard model is of course, to make predictions of what action to take by estimating the q-value for each action, given the state.
The target model tracks what action we "want" our model take.

(
But, how do we know what action we "want" the model to take, how is this defined(?) - well this network is in charge of retrieving the target values for the cost function in the standard model. This model in particular is in charge of calculate the atleast-as-accurate q-values after an action is completed (reward + discounted long term reward) we are able to use the standard model's predictions to update weights when compared to the target model's q-values, i.e. the target. Then, apply a slower weight but similar weight change to the target model, allowing our standard model to train while slowing adjusting it in the target model. Remember, the target model will have the up-to-date q-value after an action is taken, (ASSUME) so this will always be a better/good estimate for the q-value (therwise the standard model would be using wrong targets), but having the target model have a slower weight update in the same direction but lower magntiude of the standard model, will allow our standard model to settle on a target and converge, while the target model incorporates the change too.
)

For simple environments, convergence is usually possible, but for complex environments, due to training "on the fly" using a single network means we are essentially changing the "goal" at each timestep, i.e. the "optimum weights" that would mean optimum behaviour/action in this state would be different for each state and would change to quick.
So divergence is because of the lack of clear direction in which to employ the optimiser, i.e. the target is changing for the inputs.
So, we have a network that changes weights slower, we slow the change towards the goal that tracks our eventual goal (role of the target model), slow enough for us to be able to move along with it. Then we have another network that is trying to acheive those goals (role of the standard model).

Training occurs in 3 main steps: remembering, learning and reorienting goals.

1) remembering: add to the memory variable, this consists of the current state, action taken, reward for this action, the new state and the done condition

2) train: the main body of the DQN, where we make use of the stored memory and actively learn from history. Start by taking a random sample of the memory. Remember, that our sample will be a certain size, called batch size. This will mean we use the set of inputs to train our network in batches. It will take batch_size number of random values from memory to train the network with.




Deep RL
-------
Deep Reinforcement Learning introduces (deep) neural networks to solve Reinforcement Learning problems — hence the name “deep”.


Deep Q-Learning Network (DQN)
-----------------------------
Uses a Convolutional Neural Network to approximate q-values.

In reinforcement learning, convolutional networks can be used to recognize an agent’s state; e.g. the screen that Mario is on, or the terrain before a drone. That is, they perform their typical task of image recognition.

So they use the image of the game, to see what actions to take. 
But would there be a simpler way of just using actual game information from observations for their estimate calculations instead?

SO instead of numerous runs to update the Q-table, we would have numerous runs to update the weights of our network!


---