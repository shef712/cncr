TODO
----
*DONE* 1) enumerate all actions

4 directional buttons
2 down block (down+left and down+right)
(don't need up-combo buttons because we can do diagonal jumps from consecutive horizontal and jumping button presses)
TOTAL (DIRECTION = 6

6 combat buttons (punches and kicks)
combination of one directional and one combat button = 24
TOTAL (COMBAT) = 30

empty action (zero element array because doing nothing might be the best action)

TOTAL(ALL) = 37 action arrays


*DONE* 2) pre-process the data

Range of values is 0 to 1, we can shift to -0.5 to 0.5 by subtracting the normalised values by 0.5.

(4) position (x - start_position)/range_x 
min x = 33554487, max x = 33554889, ground y = 192, peak-jump y = 122
when jumping, the moment the character lands the x becomes higher than max_x
when crouching, the x becomes lower than min_x
so will have to manually handle these values

(2) health / 176 (min <0, max 176)
health can be negative so will truncate to 0

(1) game clock / 99 (min 0, max 99)

(4) jumping and crouching, either 0 or 1, for each player

?(1) absolute difference in X and Y coorindates between players - LEAVE for now, will include if i have issues with convergence, need to start with as small as states as possible to speed up initial training

Get initial values for each state variable
# starting state of the sf2 environment (for all characters)
start_info_state = [33554637, 192, 33554739, 192, 176, 176, 99, 0, 0]

PROBLEM with returning the latest state from the memory, because this was pre-processed and i wasn't considering this. FIXED

POSSIBLE remove the jumping state variable as the "y" state variable could capture this information
POSSIBLE remove the health states because this agent shouldn't necessarily need to change how the agent would behave, though riskier actions if the health gap is significant could pose potential tactics, but leave for now
AS A RESULT could only have 7 state variables only
POSSIBLE remove the clock variable because the agent should not need to decide its actions based on time
THOUGH these are variables that the human player would see, so i guess should be considered for that reason alone, will leave out for now to speed things up

WELL how many states is too many, i mean surely a NN is capable of several input elements...


IGNORE
?(2) move ID for each player
i can't get the enemy move ID since i am not controlling them in any way, and i can't get anything from the the RAM memory either, controlling the enemy through versus mode is not possible either since i can't access the second player actions through open ai's environment...
- the agent will have to act without knowing the enemy's current move, the main problem with this is when the enemy is in the middle of a combat action or is doing a long-range attack, the agent will not consider this and act relentlessly
- this wont be the biggest problem if we alter the reward function such that there is no penalty for being hit, so that it wont converge to always block and never hitting since it was getting penalised for being hit, could make an aggressiver agent if time was even included in the reward function with every second going a penalty
- the agent will only use enemy position and jumping/crouching information to coordinate its actions, and essentially be an "all-attack" agent (i wonder if there would be a way to be "all-defend" and then combine the two networks?)
- as long as we show it to be a worthwhile tactic and design decision in the report, this might not waylay the project too much
- practicality it may not be amazing, since all out attack would mean that it learns the combos, and knows what moves to execute depending where the enemy is, but it will never block... unless it learns that i can execute its attacks quicker by not getting hit, and doing a combo after a successful block? theoretically yes, but would it ever block long range attacks then? maybe have some small penalty to play around with, if the player is hit, but probably not...
STILL "agent_move_ID" may still prove to be important for the ability to execute combos within frame limits

IGNORE
?(1) "move_repeat_n" which will indicate how many times the current move has been repeated, with the hope that the agent repeats moves for certain states,
since there is a penalty for being hit, if a different action than the original is rewarded for hitting the enemy, then this would not quickly rectify because another action different to the crap original could get penalised or even rewarded if it was the original good action, all this would mess up the network training as the rewards/penalties are not accurately awarded
SO i am not going to use this idea at all in fact, since it doesn't seem like it will converge
POSSIBLE feed in last several actions indexes to get network to output an action that considers this? though this would be the state becoming that much longer, so don't think this will work, NO 


*DONE* 3) find a way for the agent to learn to repeat actions (assuming this is needed) to execute combat buttons, especially to execute complex attacks

During a combat action, which takes more than one frame to complete, if we allow other actions to execute during this frame window of the original action, then we risk messing up our network because incorrect actions will be rewarded. So we need to make sure actions finish completely before stepping with another action.

Similarly, when the current action is to jump the agent will start their jump in the following frame. During the 46 frames that a player is jumping, other actions can happen, but only one combat action is allowed, and all other presses will have no effect and we risk the same problem of other actions receiving incorrect rewards.

The directional buttons don't affect this but this is understandable since these buttons (except for UP) can last a single frame in this setting, but we can define a minimum frame window to execute these actions, probably something low like 4-8 frames, so we hold the direction for 1/10 or 1/5 of a second, which should be be low enough to execute complex moves, will chose 4 to start off with as hadoukens work with this frame window for its directions. We seem to be running the game in 40fps (40 frames occur in 1 second of the game clock).
We will also repeat the action and accumluate the reward for the frame window, which should be 0 mostly until the reward for hitting the enemy comes in one of the frames (if it does).

We can the train the network after each action rather than each frame.
We can't set a frame window and execute actions in these windows, because a constant frame window will be inelegible when a heavy kick takes 17+ frames to execute but we may need less for a combo to work, as well as these values differing with each character. 
Each button press cannot be given a frame window because it differs with each character and move too, which i am not willing to do.
Mainly, if we let the agent do other actions after the original action frame window, if the original action is not complete, which is likley to happen, then actions following the original would recieved incorrect rewards, until the original is actually finished (same problem we are used to about incorrectly training the network).

CURRENT IDEA
We can use the state variable "agent_combat_active" to determine when a combat action has been completed. This can ultimately determine when the agent is "active" in executing a combat action and 0 otherwise. 
THIS will allow be to restrict movements at applicable times so the network is not misled.
We can use "agent_jump" in a similar way to determine when the agent has finished jumping.
As a result, we are able to execute a new action only when the last action has been completed, where hopefully, chaining combinational buttons should still be possible, since the start of the next combo will always be directional... if that makes sense
NEEDS TO BE TESTED, but this variable gives valuable information to possibly solve this problem now! TESTED and works consistently
AS WELL AS "enemy_combat_active" provides more information that will possibly help the agent to block when the enemy is about to attack, where the position of the agent and enemy along with if the enemy is attacking should be a indicator of when to block.

Jumping will have to be handled differently, because only one action is allowed during the 46 frames that a jump takes. Where an action could expire naturally while in the air (early action) and be interrupted when landing (late action), both have their benefits and are valid actions. Note, if the agent has done an early action, it is not allowed to do another action, even if there is enough frames to do so, so we should restrict actions until the agent is no longer jumping.
TEST if agent_combat_active is 1 when an action has been done AND EXPIRED while in the air, which means we will not need to restrict actions while in the air since agent_combat_active is 1 CONFIRMED only when agent_y is returned to normal does agent_combat_active go down to 0, when it definitely expired much before the agent lands again


ALTERNATIVE - NO
why could't we make an action last 20 frames? well because complex moves like hadoukens would not work, and if the action takes 17 frames, then surely it would refire on the 18th frame and mess everything up by being repeated.
ALTERNATIVE - NO
Give an action at every 20 frames, which would work because a move doesnt last more than 20 frames, so we could step a new action at every 20 seconds, though this would not work for complex moves that need directional input beforehand.

NOTE we tested fps of our env (1 second in game clock) with:
# print("timestep = ", step, ", clock = ", info["clock"])


IMPLEMENTATION ->
we can change trial_length to count the number of actions, instead of number of timesteps/frames
so if trial_len = 500, we will be executing 500 actions in a trial
note, we will still be stepping through the environment at every frame, but will wait until either the agent is no longer active or min_frame_window amount of frames has been stepped through, and only then wlll the next action be undertaken,
a direction button will for a min_frame_window, 
NOTE we don't need to handle UP seperately because after min_frame_window frames another action can be made availble since agent_combat_active will be 0, and the combat_active will automatically last when an action is taken, which will last until the player lands straight up again
we will need to have a for loop to step through the environment, which means we will be running trial_len * total_frames_to_finish_all_actions... which will be a lot of frames since min_frame_window is 5, so 2000 frames minimum right now... will start off with 50 actions per trial
as the for-loop steps through the environment, it will accumulate the reward, which will usually equal 0 until the one frame where the enemy would be hit (where some combat presses live a standing heavy kick will do two kicks hits and thus the accumulation of reward will be come mainly from both frames in the total amount of frames for this action, which is fine)
QUESTION how will a complex action be given a reward, because surely the last action will be given the reward if it is in some certain state, i mean, if we are doing a hadouken, the last button press is a punch button, so for a certain state, it will do a normal punch rather than the directional buttons beforehand needed to do the hadouken that actually got the reward 
SO the state needs to include some idea of the last few moves done (i.e. directional buttons) to output the hadouken when the delta_position is high, rather than just an ordinary punch, which it will do in the current state definition that doesn't include the previous moves
ANSWER will include the last 3 moves pressed...
ALTHOUGH this will not chain complex moves together, but hopefully it will do the most powerful moves in these situations, so a shoryuken if the enemy is right next to agent, rather than a light punch
THEN providing a more previous moves would mean the agent is capable of complex combos hopefully, including super move (which may require the "meter" variable, will do if there is time)

If we are going to provide the last 3 indexes of the agent's actions (probably normalised too by diving by actions_n), the NN will consider this by updating the reward of the state with the 3 actions taken in the past, such that if the agent is in that state once more, it will eventually know that a punch would be beneficial if the delta_position is high, both players are standing, and the last 3 actions were the directional buttons to perform the hadouken.
How will the first 3 actions provide input to the NN when there are not 3 actions to use as input? Can't very well put empty-action cos this will mess up the network training if the 4 action hits. I guess, we can just use our model to predict actions for the first 3 actions, but not actually train with it, then that way the 4th actions will use reliable actions (especially if the NN is already outputting actions that have accurate q-values already), so the first 3 actions do not aid in training basically, thats fine

NOTE passing in the last 4 states to the network is what the atari solution does, so if it looks like training isn't too bad with a minimised state length, then try this to see if there is improved performance OR if convergence doesn't look good even with full state variables, then maybe passing in last 4 could help


The agent combat active should be 1 in the same timestep as the action that we step through the combat action with.
In the last action frame, the action should not affect the agent_combat_active if it steps through and turns inactive for the original frame TEST

Takes 33 frames for a heavy kick... 113 - 80 = 33, where the 112 frame is where the new state becomes inactive, so we count this as an active frame we can have stepped through the environment.


PROBLEM all actions were taking one frame and agent_combat_active was never 1 FIXED because the start of the RyuVsKen state had the "FIGHT!" message on screen where no players could do anything yet, which is why loads of actions were happening over one frame LESSON should see behaviour of error on long term sometimes

PROBLEM following combat action wasnt executing FIXED added a one frame delay by stepping through with an empty action, wont be added to the network and one frame shouldnt make a big difference to the game


NOTE remember to alter hyper parameters to see if convergence is improved in any way

NOTE do i need combinational buttons? this would just mean i need fewer previous moves pressed if we combine the last directional with the combat, but single buttons seem to give better performance for the hadouken, and it would mean a lot less action outputs... one more input state variable though, try both! Empty action would only be useful if our reward function reward for maintaining the health gap, which would be could for early game? or if the agent's health was lower and it should be defensive...

Remember the current reward function gives reward for score only.
TEST if bonus score is being given to last action NOPE there is no score from end game (bonus or from winning the match) given in any way, the score memory address is strictly for in-match score recieved, good.
DO NOT TRAIN network when the results of the match are unfolding, i.e. in between rounds when the frames, so stop training when clock is 0

The "done" clause will fire when the game over screen is shown in CHAMPIONSHIP mode only, the versus one will not be applicable here which i can use number of actions to terminate a trial.

CHANGED scenario.json to cause a "done" on "continuetimer" = 9 INSTEAD of 10


*DONE* 4) use saved weights to load a network while rendering the environment
- check model is saved and loaded correctly, i.e. same weights, 
= seems like it to me

*DONE* 5) decrease the network, start with one layer and a lower amount of neurons

*DONE* 6) will need to alter the reward function possibly, because it does not get a penalty for getting attacked (CONFIRM), and time needs to be included in this function, which i can do later
= score is enough to start seeing results now

*DONE* 7) get better indicators on how network is progressing in its training, what is a goood indicator for the network?
- score per trial would be sufficient, we can see how much score it is getting, which encompassed how many games won per trial, so as we complete more and more trials, we expect a high score due to better behaviour, current reward function gives reward from score, so if we were too encompass penalties for losing health,then this indicator would still be sufficient

---
THINGS to change if convergence isn't great:
- number of neurons and/or layers
= using a one-layer network of 9 neurons, this should be good enough

- hyperparameter values

- reward function (e.g. score, health gap)

- modifying (probably adding to give more information to agent) state variables = possibly add in the last 4 states as a extreme
= adding in previous move indexes (normalised of course), last 4 to start off with, but the super moves and the more complex combos will require more... (ryu's highest combo is 10 moves, so the previous 9 would be needed for that), medium may be using 20 which is more than enough for me

- adding in combiantional actions
===
reward functions: 
- ADDED penalty for being hit
SHOULD have penalty in respective to amount health lost, but will have fixed penalty for now

INCREASED frame window for movement, but now it won't do execute a hadouken when a movement takes so long...
IDEALLY i need a way to allow movement frames to be dynamic, so i can execute hadoukens with small amount of frames... or maybe just settle with basic movement to win games? this could be an advantage, saying this is the power of the agent which doesn't execute complex commands... do i need as many past action move idexes then? possibly not
DECREASED to 4 because it should work on 4 still surely if movement is best for a set of states

PROBLEM
you get score for breaking the objects in the background...
and you get no score if the player blocks, which should really be encouraging the player instead of "no effect" on reward for the player, so i will use health difference as reward definiton, especially because breaking objects gives you rewards
using the health gap function, we hope that the agent will execute actions that move the agent closer to the enemy to attack and reduce the health gap, since this will yield the most long term reward
similar, always to attack the enemy to increase a positive health gap
on the other hand, maximising score should have made the agent attack the enemy too, but the score from breaking objects would have derailed the network (and may have done so already)
health is an important part of state because we do not need the agent attacking so recklessly when it has so much higher health
similarly, we do not need it to tackle so much with higher health when time will run out, so clock should be included too
OK


the "done" variable might have trouble firing because i am skipping frames and stuff, so i need to make sure i step through in every loop to get the latest done variable
AND should only train when the agent is non-active essentially, not if it is non-active due to round/match end
DONE

TEST if a step with one frame ever occurs now, it does and i'm not sure why...

How can i handle the case of being stunned, where i shouldn't be training at this point, how do stuns work in sf2 anyway...
FOUND STUN variable, do not train if stunned
This value is 256 in the buildup animations of being stunned, which is the same thing as being stunned in my eyes, i dont think this value is 256 when the stun meter is building but has not reached, but 256 when confirmed stun but is the value when the stars arent shown and the agent is still falling, but it has triggered its stun so when agent gets up, they will be stunned, with stun = 1
DONE

adding in combination actions
DONE
using more previous actions
DONE
- going to run for 1000 trials and improvement (or lack of) made, then go to new reward function finally
= i wonder if too much training due to the many 4 frame directional actions is causing some issue?
TEST with both 4 frames and 20 frames
20 frames - got worse in 500 trials
testing 4 frames with less epsilon decay, penalty is also relative to health lost
DONE

adding in health reward functional
- reward for not getting hit may help a lot 
DONE

check if epsilon is going to min to quickly.... maybe go down after each trial?
DONE

fixed bug 
SO re run current reward function before changing to health function
DONE

ALSO might have standard action list, don't really need combinational i feel like
DONE

Health reward function
- calculate the difference and always give the reward, even if the health gap is unchanged, which may make the deciding factor for this function
(reward is not applied frame by frame, but after every action, which is the RL way)
DONE

- increase epsilon decay
- standard moves
DONE

= how will the health reward function work?
if negative health gap, all moves that do not decrease this gap will have negative reward
but, what about directional moves to move closer to the enemy? technically this will get a negative reward as well.. though at least this will have the "best" long term reward, and thus the action to mvoe towards the enemy will have the most reward!
DONE

Try and get continue timer to work, because then we can have actual rounds as part of the trial
FIXED

Is there a clever way to make the player execute hadoukens?

directional action(s) will execute in between rounds
FIXED

Change batch size?

RYU won two matches and lost third on trial 36!
I THINK a network against each different enemy would help, which makes sense since a different play style is needed against each character

Maybe it needs more layers to combat the different player styles...
DONE

"Rounds won" resets for each match, so increment every time rounds win goes 
CHECK if actions were being executed when moving to next character

what if a reward function that gave massive reward for hurting the enemy, and massive penalty for losing health?
forget score and health gap?

create a folder for each character, and a model for each enemy character?

technically the hadouken wont get rewarded properly because the directions leading up the hadouken dont get any reward, though they wont get a penalty assuming they arent hit while pressing the actions, and if the previous directions are pressed than surely the network will be clever enough to output the next reward (punch) to attack the enemy and get reward, ideally anyway

playing with hyperparameters:
caring more about long term reward
quicker epsilon decay
increase learning rate
greater update of weight changes to target network weight changes

- decrease epsilon decay slightly

decrease actions list to standard again, with memory_actions_= 2 -> 3
try batch size decrease to 24 as well
DONE

is there a way to reward for blocking?
well health gap helps for now...
DONE

need a better reward indicator of over matches, so do total reward from damage dealth, i.e. enemy health lost 
DONE

CREATED figure_3.png


does the "up" mess up the network in any way?
DONT THINK SO cos the agnet is combat active 

monday = write first draft abstract, into and plan for background
can change abstract when we get results...

method can easily be our setup for our agent, results is most important then...

plot epsilon line as well, to see its effect, may need to construct a different linear or quadratic function for epsilons value over trials


for training, i should keep epsilon slowly trail for until 500-750 trials for a 1500 trial length
DONE

        self.gamma = 0.9
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.997 # 0.97, 0.997

        self.learning_rate = 0.015 # 0.005
        self.tau = 0.2 # 0.125
Figure 4


recheck DQN logic...
compare with other sources

another metric for improvement over random agent is amount of time agent survives per trial

pass in previous 4 states now, this may help...



increase memory to hold last 5000 steps
should mean we always have a bigger wealth of experience to train from
what if memory is full, do new memories get added in? yes
maybe we shouldn't train from the bad games to much, and only train if the reward is a certain level?
though RL would dictate that these actions would not be chosen then...

reduce unecessary states like: agent_jump?

PROBLEM
that the agent is on the floor or hit and is trying tp execute valid moves!
when executing combat moves, then agent_enemy_combat is fine, because it knows it cannot execute the move for the full frames
but directionals are deemed as valid when the agent is on the floor
using agent_hit variable to execute actions or not
DONE

performance looks better at early stages of training...
i wonder if it is best to stop training when the agent behaves well, though i cant tell that with epsilon being so high...
I'll have to hope that the different circumstances of our enemy attacking doesnt mess up our agent's behaviour... surely it'll pick the option that is best for it after experiencing so many matches with the enemy

still does hadoukens at min_frame_window = 40


NEED to run several tests of why my agent is better (or not) than my agent
Use same characters
Tests for time survived (plot average match time), damage dealth per match,
which would paint a picture of agent performance

Try testing with M Bison, the most OP character...

Talk about common techniques the agent has learned:
- anti-air
- downslide (for bison) as soon as game start_info_state
show pictures (3/4 frames to show activity)

All this needs to come from trained agent...


Train for 50 trials, with epsilon decaying to min around trial 40.
Test how good this network would do.
If similar behaviour to random agent, make sure to say trained on x states, and y actions and 50 trials.... may do 100 trials if it looks better for "training" purposes

Email morteza and say results are okay, definitely not "beaten" the game but performance is good, relaly good results have been difficult, ive expanded the network to include 4 previou states and actions, etc but still this is the result
Going to try 20 states, and see 

trial 25, 35 and 40 look good to me...

MBison Folder for above


Increasing gamma, and decreasing epsilon decay, increasing tau
Memory 1000 for 100 batch size

When i get good results i will compare the effects of changing variables


How about a really quick epsilon decay and then see its peformance?


TRIAL 30-35 LOOKS GOOD, a lot of slide downs and anti-airs
(check ryu)

Its winning a lot of rounds from 30-40, maybe just chnage epilson decay and use this agent?

40+ seems good too, spams down slides

50+ is good too, seems to do better in following matches too..

60+ too, seems to love the corner slide and mid kick

70+ is winning matches, still decent performance, but 75+ seems to be doing worse... (it usually wins when the corner trap can happen, otherwise not as well)
its still winining matches i guess
maybe around 60-70 is the sweet spot?

if it is diverging,  maybe decrease tau or alpha?
since performance may be doing worse

okat, 80+ still winning, maybe a bit worse against guile, but better against ken...
trial 88 still corner trapped guile and perfected him in a round, and doing decent against ken (anti air)

would more layers help the agent against other characters?

trial 90 still tries to corner trap, and blocks ocassionaly, anti air is there and it wins rounds still so i guess its okay,
still doesnt win againt ken very well, maybe say that the network wins a lot more against guile and not on ken due to the network being fitted for guile, and should be trained on all characters
we will try by adding my layers...

Figure 5 is above
2-trial

NOW, added a second layer to see how well it does against other characters in particular

Trial 10-15 is really good, winning 5 matches...
10-20 seems to be perfecting the slide down move
20+ corner trap seems to be forming
late 20's corner trap doing well (mix of slide and mid kick)
30+ corner and anti air going strong
30-40 winning a lot of matches.... at epsilon 0.3

TRY THIS with ryu

40-50 seems to win a lot of matches too

60+ seems to have learn corner trapping well but absolutely flops against ken

70+ winning rounds but no matches, is it doing worse against guile now?
Nah its still beasting against guile, and against ken is not bad but still losing

I wonder if it would have been better to train against each character 50 times?
resetting the epsilon?

and then testing against the arcade?

first try as ken as the first opponent and see if the agent beats ken as well as guile

up to 90, performance against guile is good...

that way i can say we trained bison against each character and here is the performances against each
use either matches or rounds as performance indicator, whichever is better
(need to get other statisitcs as well, to compare against random agent)

agent would be really powerful if we could train on each character each time for 50 trials, and then it doing as well each time...

try for guile, ken and chunli for now...

90-100 really good performance against guile
not bad against ken...

figure 6 above


load and see if good performance still


clearly show_network isnt working, so after trial finish, just run then...

THINGS TO DO:
- run current network with ryu 
- run network against ken and chun li and see the difference in matches won against them
- set up graphs and tests for the trained agent after training, 100 trials only


---
- run random agent and use this as a comparison to our agent, can do this for directional actions too

- come up with a research question, e.g. the effect of this variable or method on this environemnt, possibly applyong DQN on a complex environment sf2, or even just a interesting variable in sf2 that may increase convergence (if possible), effect of reward function...
-------
ARCHIVE
-------

actions = [
    # directions only
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],

    # combat only
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],

    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]

actions = [
    # directions only
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 
    [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], 
    [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],

    # combat only
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    
    # direction + combat
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
    
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
    
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
    
    [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    
    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
    [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    
    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
    
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]



                    # print("TRAINING at --- STEP ", action_step)


            # print("--- STEP ", action_step, "(", action_array , "), TOOK ", frame_count, " FRAMES, REWARD = ", total_action_reward, " ---")
            


                