TODO
----
*DONE* 1) enumerate all actions

4 directional buttons
2 down block (down+left and down+right)
(don't need up-combo buttons because we can do diagonal jumps from consecutive horizontal and jumping button presses)
TOTAL (DIRECTION = 6

6 combat buttons (punches and kicks)
combination of one directional and one combat button = 24
TOTAL (COMBAT) = 30

empty action (zero element array because doing nothing might be the best action)

TOTAL(ALL) = 37 action arrays


*DONE* 2) pre-process the data

Range of values is 0 to 1, we can shift to -0.5 to 0.5 by subtracting the normalised values by 0.5.

(4) position (x - start_position)/range_x 
min x = 33554487, max x = 33554889, ground y = 192, peak-jump y = 122
when jumping, the moment the character lands the x becomes higher than max_x
when crouching, the x becomes lower than min_x
so will have to manually handle these values

(2) health / 176 (min <0, max 176)
health can be negative so will truncate to 0

(1) game clock / 99 (min 0, max 99)

(4) jumping and crouching, either 0 or 1, for each player

?(1) absolute difference in X and Y coorindates between players - LEAVE for now, will include if i have issues with convergence, need to start with as small as states as possible to speed up initial training

Get initial values for each state variable
# starting state of the sf2 environment (for all characters)
start_info_state = [33554637, 192, 33554739, 192, 176, 176, 99, 0, 0]

PROBLEM with returning the latest state from the memory, because this was pre-processed and i wasn't considering this. FIXED

POSSIBLE remove the jumping state variable as the "y" state variable could capture this information
POSSIBLE remove the health states because this agent shouldn't necessarily need to change how the agent would behave, though riskier actions if the health gap is significant could pose potential tactics, but leave for now
AS A RESULT could only have 7 state variables only
POSSIBLE remove the clock variable because the agent should not need to decide its actions based on time
THOUGH these are variables that the human player would see, so i guess should be considered for that reason alone, will leave out for now to speed things up

WELL how many states is too many, i mean surely a NN is capable of several input elements...


IGNORE
?(2) move ID for each player
i can't get the enemy move ID since i am not controlling them in any way, and i can't get anything from the the RAM memory either, controlling the enemy through versus mode is not possible either since i can't access the second player actions through open ai's environment...
- the agent will have to act without knowing the enemy's current move, the main problem with this is when the enemy is in the middle of a combat action or is doing a long-range attack, the agent will not consider this and act relentlessly
- this wont be the biggest problem if we alter the reward function such that there is no penalty for being hit, so that it wont converge to always block and never hitting since it was getting penalised for being hit, could make an aggressiver agent if time was even included in the reward function with every second going a penalty
- the agent will only use enemy position and jumping/crouching information to coordinate its actions, and essentially be an "all-attack" agent (i wonder if there would be a way to be "all-defend" and then combine the two networks?)
- as long as we show it to be a worthwhile tactic and design decision in the report, this might not waylay the project too much
- practicality it may not be amazing, since all out attack would mean that it learns the combos, and knows what moves to execute depending where the enemy is, but it will never block... unless it learns that i can execute its attacks quicker by not getting hit, and doing a combo after a successful block? theoretically yes, but would it ever block long range attacks then? maybe have some small penalty to play around with, if the player is hit, but probably not...
STILL "agent_move_ID" may still prove to be important for the ability to execute combos within frame limits

IGNORE
?(1) "move_repeat_n" which will indicate how many times the current move has been repeated, with the hope that the agent repeats moves for certain states,
since there is a penalty for being hit, if a different action than the original is rewarded for hitting the enemy, then this would not quickly rectify because another action different to the crap original could get penalised or even rewarded if it was the original good action, all this would mess up the network training as the rewards/penalties are not accurately awarded
SO i am not going to use this idea at all in fact, since it doesn't seem like it will converge
POSSIBLE feed in last several actions indexes to get network to output an action that considers this? though this would be the state becoming that much longer, so don't think this will work, NO 


*DONE* 3) find a way for the agent to learn to repeat actions (assuming this is needed) to execute combat buttons, especially to execute complex attacks

During a combat action, which takes more than one frame to complete, if we allow other actions to execute during this frame window of the original action, then we risk messing up our network because incorrect actions will be rewarded. So we need to make sure actions finish completely before stepping with another action.

Similarly, when the current action is to jump the agent will start their jump in the following frame. During the 46 frames that a player is jumping, other actions can happen, but only one combat action is allowed, and all other presses will have no effect and we risk the same problem of other actions receiving incorrect rewards.

The directional buttons don't affect this but this is understandable since these buttons (except for UP) can last a single frame in this setting, but we can define a minimum frame window to execute these actions, probably something low like 4-8 frames, so we hold the direction for 1/10 or 1/5 of a second, which should be be low enough to execute complex moves, will chose 4 to start off with as hadoukens work with this frame window for its directions. We seem to be running the game in 40fps (40 frames occur in 1 second of the game clock).
We will also repeat the action and accumluate the reward for the frame window, which should be 0 mostly until the reward for hitting the enemy comes in one of the frames (if it does).

We can the train the network after each action rather than each frame.
We can't set a frame window and execute actions in these windows, because a constant frame window will be inelegible when a heavy kick takes 17+ frames to execute but we may need less for a combo to work, as well as these values differing with each character. 
Each button press cannot be given a frame window because it differs with each character and move too, which i am not willing to do.
Mainly, if we let the agent do other actions after the original action frame window, if the original action is not complete, which is likley to happen, then actions following the original would recieved incorrect rewards, until the original is actually finished (same problem we are used to about incorrectly training the network).

CURRENT IDEA
We can use the state variable "agent_combat_active" to determine when a combat action has been completed. This can ultimately determine when the agent is "active" in executing a combat action and 0 otherwise. 
THIS will allow be to restrict movements at applicable times so the network is not misled.
We can use "agent_jump" in a similar way to determine when the agent has finished jumping.
As a result, we are able to execute a new action only when the last action has been completed, where hopefully, chaining combinational buttons should still be possible, since the start of the next combo will always be directional... if that makes sense
NEEDS TO BE TESTED, but this variable gives valuable information to possibly solve this problem now! TESTED and works consistently
AS WELL AS "enemy_combat_active" provides more information that will possibly help the agent to block when the enemy is about to attack, where the position of the agent and enemy along with if the enemy is attacking should be a indicator of when to block.

Jumping will have to be handled differently, because only one action is allowed during the 46 frames that a jump takes. Where an action could expire naturally while in the air (early action) and be interrupted when landing (late action), both have their benefits and are valid actions. Note, if the agent has done an early action, it is not allowed to do another action, even if there is enough frames to do so, so we should restrict actions until the agent is no longer jumping.
TEST if agent_combat_active is 1 when an action has been done AND EXPIRED while in the air, which means we will not need to restrict actions while in the air since agent_combat_active is 1 CONFIRMED only when agent_y is returned to normal does agent_combat_active go down to 0, when it definitely expired much before the agent lands again


ALTERNATIVE - NO
why could't we make an action last 20 frames? well because complex moves like hadoukens would not work, and if the action takes 17 frames, then surely it would refire on the 18th frame and mess everything up by being repeated.
ALTERNATIVE - NO
Give an action at every 20 frames, which would work because a move doesnt last more than 20 frames, so we could step a new action at every 20 seconds, though this would not work for complex moves that need directional input beforehand.

NOTE we tested fps of our env (1 second in game clock) with:
# print("timestep = ", step, ", clock = ", info["clock"])


IMPLEMENTATION ->
we can change trial_length to count the number of actions, instead of number of timesteps/frames
so if trial_len = 500, we will be executing 500 actions in a trial
note, we will still be stepping through the environment at every frame, but will wait until either the agent is no longer active or min_frame_window amount of frames has been stepped through, and only then wlll the next action be undertaken,
a direction button will for a min_frame_window, 
NOTE we don't need to handle UP seperately because after min_frame_window frames another action can be made availble since agent_combat_active will be 0, and the combat_active will automatically last when an action is taken, which will last until the player lands straight up again
we will need to have a for loop to step through the environment, which means we will be running trial_len * total_frames_to_finish_all_actions... which will be a lot of frames since min_frame_window is 5, so 2000 frames minimum right now... will start off with 50 actions per trial
as the for-loop steps through the environment, it will accumulate the reward, which will usually equal 0 until the one frame where the enemy would be hit (where some combat presses live a standing heavy kick will do two kicks hits and thus the accumulation of reward will be come mainly from both frames in the total amount of frames for this action, which is fine)
QUESTION how will a complex action be given a reward, because surely the last action will be given the reward if it is in some certain state, i mean, if we are doing a hadouken, the last button press is a punch button, so for a certain state, it will do a normal punch rather than the directional buttons beforehand needed to do the hadouken that actually got the reward 
SO the state needs to include some idea of the last few moves done (i.e. directional buttons) to output the hadouken when the delta_position is high, rather than just an ordinary punch, which it will do in the current state definition that doesn't include the previous moves
ANSWER will include the last 3 moves pressed...
ALTHOUGH this will not chain complex moves together, but hopefully it will do the most powerful moves in these situations, so a shoryuken if the enemy is right next to agent, rather than a light punch
THEN providing a more previous moves would mean the agent is capable of complex combos hopefully, including super move (which may require the "meter" variable, will do if there is time)

If we are going to provide the last 3 indexes of the agent's actions (probably normalised too by diving by actions_n), the NN will consider this by updating the reward of the state with the 3 actions taken in the past, such that if the agent is in that state once more, it will eventually know that a punch would be beneficial if the delta_position is high, both players are standing, and the last 3 actions were the directional buttons to perform the hadouken.
How will the first 3 actions provide input to the NN when there are not 3 actions to use as input? Can't very well put empty-action cos this will mess up the network training if the 4 action hits. I guess, we can just use our model to predict actions for the first 3 actions, but not actually train with it, then that way the 4th actions will use reliable actions (especially if the NN is already outputting actions that have accurate q-values already), so the first 3 actions do not aid in training basically, thats fine

NOTE passing in the last 4 states to the network is what the atari solution does, so if it looks like training isn't too bad with a minimised state length, then try this to see if there is improved performance OR if convergence doesn't look good even with full state variables, then maybe passing in last 4 could help


The agent combat active should be 1 in the same timestep as the action that we step through the combat action with.
In the last action frame, the action should not affect the agent_combat_active if it steps through and turns inactive for the original frame TEST

Takes 33 frames for a heavy kick... 113 - 80 = 33, where the 112 frame is where the new state becomes inactive, so we count this as an active frame we can have stepped through the environment.


PROBLEM all actions were taking one frame and agent_combat_active was never 1 FIXED because the start of the RyuVsKen state had the "FIGHT!" message on screen where no players could do anything yet, which is why loads of actions were happening over one frame LESSON should see behaviour of error on long term sometimes

PROBLEM following combat action wasnt executing FIXED added a one frame delay by stepping through with an empty action, wont be added to the network and one frame shouldnt make a big difference to the game


NOTE remember to alter hyper parameters to see if convergence is improved in any way

NOTE do i need combinational buttons? this would just mean i need fewer previous moves pressed if we combine the last directional with the combat, but single buttons seem to give better performance for the hadouken, and it would mean a lot less action outputs... one more input state variable though, try both! Empty action would only be useful if our reward function reward for maintaining the health gap, which would be could for early game? or if the agent's health was lower and it should be defensive...

Remember the current reward function gives reward for score only.
TEST if bonus score is being given to last action NOPE there is no score from end game (bonus or from winning the match) given in any way, the score memory address is strictly for in-match score recieved, good.
DO NOT TRAIN network when the results of the match are unfolding, i.e. in between rounds when the frames, so stop training when clock is 0

The "done" clause will fire when the game over screen is shown in CHAMPIONSHIP mode only, the versus one will not be applicable here which i can use number of actions to terminate a trial.

CHANGED scenario.json to cause a "done" on "continuetimer" = 9 INSTEAD of 10


*DONE* 4) use saved weights to load a network while rendering the environment
- check model is saved and loaded correctly, i.e. same weights, 
= seems like it to me

*DONE* 5) decrease the network, start with one layer and a lower amount of neurons

*DONE* 6) will need to alter the reward function possibly, because it does not get a penalty for getting attacked (CONFIRM), and time needs to be included in this function, which i can do later
= score is enough to start seeing results now

*DONE* 7) get better indicators on how network is progressing in its training, what is a goood indicator for the network?
- score per trial would be sufficient, we can see how much score it is getting, which encompassed how many games won per trial, so as we complete more and more trials, we expect a high score due to better behaviour, current reward function gives reward from score, so if we were too encompass penalties for losing health,then this indicator would still be sufficient

---
THINGS to change if convergence isn't great:
- number of neurons and/or layers
- hyperparameter values
- reward function (e.g. add penalties for losing health; penalty for every clock second lost if health was lower than enemy (to help against "time-out"; use difference in health as reward where equal more health is reward and negative different is penalty, where every frame/action that the health gap is negative it would get the negative reward relative to this gap, which would would tackle the "time-out" problem)
- modifying (probably adding to give more information to agent) state variables = adding in the last 4 states as a extreme
- adding in previous move indexes (normalised of course), last 4 to start off with, but the super moves and the more complex combos will require more... (ryu's highest combo is 10 moves, so the previous 9 would be needed for that)
... 
using a one-layer network of 9 neurons, this should be good enough...
---
Adding in the last 3 moves as state variable
- performance after 100 trials was bad
- run for 1000 trials to see performance
- might need to change reward funciton... get penalty for being hit maybe...
- might do exacly what medium article did, cos this may not work, see after 1000 trials with currents settings
- need to compare to random agent's score to see if my agent is even better!
- should grab random agents behaviour (avergaed) for x trials 
- the beginning trial rewards seem to be important because a lower reward in these early trials are affecting learning, probably because no penalty for being hit means that i am not really "crossing" off actions to do in certain states, and i am just waiting to find the better action (better than o probably) which will then get chosen more likley (unless epsilon says otherwise...), should tinker with reward function asap, leave network setup for now
penalty should be there, other env's ive worked with have it, so look into it!

ALSO start writing up the report (intro and background or something, though this would be nice having some results to work against... still can do it though, at least background anyway)

---
reward functions: 
- penalty for being hit: -2000?, higher than any score from an attack i've seen  of 1000 for shoryuken (red hadouken or super could be more but i'm not considering this just yet)
how can i assign penalties though? if the agent is in a starting state and performs an action, if the agent is hit then the enemy is no longer active right?so if theyre health decreases after their action has finished (interrupted) then we should assign a negative reward there,
though what if both players get hit? well we'll assign the penalty to the total reward gotten from that action anyway
SHOULD have penalty in respective to amount health lost, but will have fixed penalty for now

INCREASED frame window for movement, but now it won't do execute a hadouken when a movement takes so long...
IDEALLY i need a way to allow movement frames to be dynamic, so i can execute hadoukens with small amount of frames... or maybe just settle with basic movement to win games? this could be an advantage, saying this is the power of the agent which doesn't execute complex commands... do i need as many past action move idexes then? maybe not... though i think it helps the player move closer to the agent if it knows it hasn't moved closer to them? i dunno

maybe add in the combinational actions, could hurt i guess...


you get score for breaking the objects in the background...
and you get no score if the player blocks, which should really be encouraging the player instead of "no effect" on reward for the player, so i will use health difference as reward definiton, especially because breaking objects gives you rewards, and the rewards in code seem to multiplied by 10 where the game doesn't...
health gap after each action as reward (each frame get reward, even during attacks, well no because if the player gets hit during a kick with a flying projectile, we do not want to reward the frames of active kicking...)
where the the reward will be equal to the health gap, so for example, max reward is 176 is the player did an attack that took the entire health of the enemy in one action
where maintaining the health gap, i.e. doing an action and losing health will also be rewarded with the health gap amount.
When the health gap is negative, for every action the agent does that maintains the negative health gap or increases it, the action will get a higher penalty, where actions that decrease the health gap will get a slighly less reward (which is a bit lame) but it might help... should implement

using the health gap function, we hope that the agent will execute actions that move the agent closer to the enemy to attack and reduce the health gap, since this will yield the most long term reward
similar, always to attack the enemy to increase a positive health gap

on the other hand, maximising score should have made the agent attack the enemy too, but the score from breaking objects would have derailed the network (and may have done so already)

use medium article's reward and states, may include more states and previous actions, though they might be using 20, which would be more than enough for me

health is an important part of state because we do not need the agent attacking so recklessly when it has so much higher health
similarly, we do not need it to tackle so much with higher health when time will run out, so clock should be included too

do i really known which action is receiving the +1000 reward? possible the last action or some action in the very least, even if we don't see it
which makes me think, what happens for an action that is being executed when the agent is being hit... THINk ABOUT maybe have some way to only attack when agent_active is not active
although, surely when the agent is being hit, then agent_active is 0, so no action will be executed and nothing will be added to the network
OKAY make sure nothing is added to the network when an action isnt even executed,because there has to be some action (even an empty one) that makes the RL loop continue, which means the network will learn after every official/actual action!
- check if there is ever moments where frame_count = 1 even occurs then... YEP there is... i think these are frame where the agent can not hit and is therefore made inactive through the game... so if there is no combat or directional action active, then do not train?
WAIT if i am only training when the agent does an action, then what consequences are there? well does this mean during the time where the agent is getting hit and so it cannot execute actions, we skip these frames... which is fine, cos we are then skipping the enemy's actions frame window too
as long as the enemy can attack and reduce reward during an action, which it can because a long range attack from an enemy to the agent executing any action, should stop that action when hit with enemy's attack, and then the action is added to the network with consequential reward of 0, but if penalty for being hit if reward function defined as so
SO adding a valid action to the network should be fine, because the agent will always be executing actions and doesnt when it is not capable of doing, for which then the loop moves in single frames, but we do not add anything to the network anyway so its fine. MAKE SURE OF THIS, RE-READ and RE-EVALUATE!

TODO
Should only train when the agent is non-active essentially, not if it is non-active due to round/match ended

the score from breaking objects seems to make the agent jump up all the time in a particular training session, probably because the agent was hit before it did another action and it fell and to get the reward of the crate breaking... not what we want maate

the "done" variable might have trouble firing because i am skipping frames and stuff, so i need to make sure i step through in every loop to get the latest done variable

---

8) run random agent and use this as a comparison to our agent, can do this for directional actions too

9) come up with a research question, e.g. the effect of this variable or method on this environemnt, possibly applyong DQN on a complex environment sf2, or even just a interesting variable in sf2 that may increase convergence (if possible), effect of reward function...

FRIDAY (email morteza and show progress and all work done above, for info and for update)

BELOW (BONUS) - IGNORE

10) RELATED to 7 but may not be needed, look at atari examples for frame skipping, the reccurent network may be implemented (which may not be a problem because of keras), look into using keras functional for the NN layers



ARCHIVE
-------

# actions = [
#     # directions only
#     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 
#     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
#     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 
#     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 
#     [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], 
#     [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],

#     # combat only
#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],
#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
#     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    
#     # direction + combat
#     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
#     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],
#     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],
#     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],
    
#     [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],
#     [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],
#     [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
#     [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],
    
#     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1],
#     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
#     [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],
#     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],
    
#     [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
#     [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
#     [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
#     [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    
#     [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
#     [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
#     [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
#     [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
    
#     [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
#     [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],
#     [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],
#     [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
    
#     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]